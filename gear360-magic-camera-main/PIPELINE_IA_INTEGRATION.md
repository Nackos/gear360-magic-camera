# Pipeline IA Int√©gr√© - Documentation

## üéØ Vue d'ensemble

Ce document d√©crit l'impl√©mentation du pipeline de d√©tection IA complet qui r√©plique la fonctionnalit√© du code Python avec YOLO + MediaPipe + LSTM dans le navigateur.

## üèóÔ∏è Architecture

### Composant Principal: `IntegratedDetectionPipeline`

**Localisation:** `src/components/IntegratedDetectionPipeline.tsx`

Le pipeline int√©gr√© combine trois technologies:

1. **TensorFlow.js avec COCO-SSD** - D√©tection d'objets (√©quivalent YOLO)
2. **MediaPipe Hand Landmarker** - D√©tection et tracking des mains
3. **Classificateur de gestes custom** - Reconnaissance de gestes statiques et dynamiques

### üìä Flux de traitement

```
Vid√©o Frame
    ‚Üì
1. D√©tection d'objets (COCO-SSD)
    ‚Üì (si personne d√©tect√©e)
2. D√©tection des mains (MediaPipe)
    ‚Üì (extraction keypoints 21 points)
3. Classification gestes statiques
    ‚Üì
4. D√©tection gestes dynamiques (buffer temporel)
    ‚Üì
5. D√©clenchement commandes
```

## üîß Composants techniques

### 1. D√©tection d'objets

**Mod√®le:** COCO-SSD (MobileNet V2 Lite)
- Plus l√©ger et rapide que YOLOv8n
- Optimis√© pour le navigateur
- D√©tecte 90 classes d'objets dont "personne"

**Configuration:**
```typescript
const model = await loadCocoSsd({
  base: 'lite_mobilenet_v2'
});
```

**Seuils:**
- Confidence: 0.35 (configurable via `aiDetectionConfig.ts`)
- IOU threshold: 0.45

### 2. D√©tection des mains

**Mod√®le:** MediaPipe Hand Landmarker
- 21 points de rep√®re par main
- Support jusqu'√† 2 mains simultan√©es
- GPU-acceler√© (WebGPU quand disponible)

**Configuration:**
```typescript
minHandDetectionConfidence: 0.6
minHandPresenceConfidence: 0.6
minTrackingConfidence: 0.6
```

**Points de rep√®re (landmarks):**
```
0: Poignet
1-4: Pouce
5-8: Index
9-12: Majeur
13-16: Annulaire
17-20: Auriculaire
```

### 3. Classification de gestes

#### Gestes statiques d√©tect√©s

| Geste | Description | Logique |
|-------|-------------|---------|
| `pouce_haut` | Pouce lev√© | Pouce au-dessus, autres doigts repli√©s |
| `v_sign` | Signe V | Index + majeur lev√©s, autres repli√©s |
| `index_point` | Index point√© | Index lev√© seul |
| `paume` | Main ouverte | Tous les doigts √©tendus |
| `poing` | Poing ferm√© | Tous les doigts repli√©s |

#### Gestes dynamiques (swipes)

| Geste | D√©tection | Distance min |
|-------|-----------|--------------|
| `swipe_droite` | Mouvement horizontal ‚Üí | 15% √©cran |
| `swipe_gauche` | Mouvement horizontal ‚Üê | 15% √©cran |
| `swipe_haut` | Mouvement vertical ‚Üë | 15% √©cran |
| `swipe_bas` | Mouvement vertical ‚Üì | 15% √©cran |

**Buffer circulaire:**
- Taille: 24 frames (configurable)
- Analyse de trajectoire sur fen√™tre glissante
- Calcul d'angle et distance pour classification

### 4. Mapping de commandes

Les gestes d√©clenchent des actions via le `CommandController`:

```typescript
gestureCommandMap: {
  'pouce_haut': { action: 'like', apk: 'com.example.app' },
  'swipe_gauche': { action: 'prev', apk: 'com.media.player' },
  'swipe_droite': { action: 'next', apk: 'com.media.player' },
  'paume': { action: 'pause', apk: 'com.media.player' },
  'poing': { action: 'capture' },
  'v_sign': { action: 'selfie' },
  'index_point': { action: 'select' }
}
```

## ‚öôÔ∏è Configuration

**Fichier:** `src/config/aiDetectionConfig.ts`

### Param√®tres principaux

```typescript
detection: {
  model: 'yolov8n.pt',          // Nom symbolique (utilise COCO-SSD)
  imgSize: 640,                  // Taille d'entr√©e
  confThreshold: 0.35,           // Seuil de confiance
  iouThreshold: 0.45,            // Seuil IoU
  maxDetections: 50,             // Max d√©tections par frame
  device: 'cuda'                 // Mapp√© vers webgpu/wasm
}

pose: {
  useMediaPipe: true,
  minDetectionConfidence: 0.6,
  minTrackingConfidence: 0.6,
  smoothPose: true,
  smoothingAlpha: 0.4
}

gesture: {
  windowSize: 24,                // Taille buffer pour swipes
  stride: 8,                     // Pas d'√©chantillonnage
  model: 'gesture_lstm_v1.pt',   // Nom symbolique
  classes: [...],                // Classes de gestes
  confThreshold: 0.7,            // Seuil de confiance
  stableGestureFrames: 6         // Frames pour stabilit√©
}
```

## üöÄ Utilisation

### Activation dans l'interface

1. Ouvrir la page Camera (`/`)
2. Cliquer sur le bouton **Pipeline IA** (ic√¥ne √©toile brillante) dans les contr√¥les de droite
3. Le pipeline d√©marre et affiche:
   - Bounding boxes des personnes d√©tect√©es
   - Squelette des mains avec landmarks
   - Geste d√©tect√© en temps r√©el
   - Indicateur de traitement actif

### Int√©gration programmatique

```tsx
import { IntegratedDetectionPipeline } from '@/components/IntegratedDetectionPipeline';

<IntegratedDetectionPipeline
  videoRef={videoRef}
  isActive={isActive}
  onDetections={(detections) => {
    console.log('D√©tections:', detections);
  }}
  onGesture={(gesture, confidence) => {
    console.log(`Geste: ${gesture} (${confidence})`);
  }}
/>
```

## üé® Rendu visuel

Le composant dessine sur un canvas overlay:

- **Vert** - Bounding boxes des personnes d√©tect√©es
- **Rouge** - Points de rep√®re des mains
- **Vert** - Connexions entre les points
- **Overlay HUD** - Statut et geste d√©tect√©

## ‚ö° Optimisations

### Performance

1. **Frame skipping:** Traite 1 frame sur 2 pour r√©duire la charge CPU
2. **Buffer limit√©:** Taille maximale de 24 frames pour swipes
3. **Mod√®le l√©ger:** COCO-SSD Lite MobileNet V2
4. **GPU acceleration:** Utilise WebGPU/WebGL quand disponible

### Gestion m√©moire

- Dispose automatique des tensors TensorFlow.js
- Nettoyage du buffer apr√®s d√©tection de swipe
- Release des ressources MediaPipe lors du d√©montage

## üîÑ Diff√©rences avec le code Python

| Aspect | Python | Browser |
|--------|--------|---------|
| D√©tection objets | YOLOv8n (ultralytics) | COCO-SSD (TensorFlow.js) |
| Backend ML | PyTorch/CUDA | TensorFlow.js/WebGPU |
| Gestes LSTM | Mod√®le custom PyTorch | Classificateur heuristique |
| Performance | ~30-60 FPS | ~15-30 FPS (frame skip) |
| M√©moire | Illimit√©e | Limit√©e (browser) |

## üõ†Ô∏è D√©veloppement futur

### Am√©liorations possibles

1. **Mod√®le YOLO natif**
   - Convertir YOLOv8n en format TFJS
   - Meilleure pr√©cision de d√©tection

2. **LSTM pour gestes**
   - Entra√Æner mod√®le LSTM personnalis√©
   - Convertir en TensorFlow.js
   - Meilleure reconnaissance de gestes dynamiques

3. **Optimisations**
   - WebAssembly pour parties critiques
   - Web Workers pour traitement parall√®le
   - Quantization des mod√®les

4. **Fonctionnalit√©s**
   - Tracking multi-personnes persistant
   - Gestes √† deux mains
   - Reconnaissance de poses complexes

## üêõ Debug

### Logs console

Le composant log:
- Initialisation des mod√®les
- Backend TensorFlow.js utilis√©
- D√©tections et gestes en temps r√©el
- Erreurs de traitement

### Indicateurs visuels

- Dot vert pulsant = Traitement actif
- Texte "Pipeline IA" = Overlay activ√©
- Geste + confidence = D√©tection en cours

## üìö Ressources

- [TensorFlow.js](https://www.tensorflow.org/js)
- [COCO-SSD Model](https://github.com/tensorflow/tfjs-models/tree/master/coco-ssd)
- [MediaPipe Hand Landmarker](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker)
- [Configuration IA](./src/config/aiDetectionConfig.ts)
- [Command Controller](./src/components/CommandController.tsx)
